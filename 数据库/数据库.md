[TOC]

# 常见数据库引擎

* MyISAM
* InnoDB
* ISAM
  * 读取速度快，不占用大量内存和存储资源
  * 不运行事务，不容错，需经常备份实时数据
* MEMORY
  * 每个表可有32个索引，每个索引16列
  * 表中数据存储到内存中
* Archive
  * 为大量很少引用的历史，归档，或安全审计信息的存储和检索提供解决方案

# 如何设计一个关系型数据库

* RDBMS  
  * 程序实例
    * 存储管理
    * 缓存机制
    * SQL解析
    * 日志管理
    * 权限划分
    * 容灾机制
    * **索引管理**
    * **锁管理**
  * 存储（文件系统）
* 阶段
  * 需求分析阶段
    * 分析用户需求 
  * 概念设计阶段
    * 设计E-R图形

  * 逻辑设计阶段

      * 设计表格 
  * 物理设计阶段
    * 设计数据库的存储方式和存储路径 
  * 实现阶段 
  * 实施维护阶段

# 索引模块

## 索引的优点

* 避免全表扫描查找数据，提升查找效率
* 创建唯一性索引，保证数据库表中每行数据的唯一性
* 使用分组和排序子句进行数据检索时，减少查询中分组和排序时间
* 可在查询中使用优化隐藏器，提高系统性能

## 什么样的信息能成为索引？

* 主键，唯一键，普通键等让数据具备**一定区分性的字段**
* 什么情况下适合建立索引
  * 经常出现在关键字`order by` ,`group by `, `distinct` 后面的字段
  * union等集合操作的结果集字段
  * 经常查询的字段
  * 经常用作表连接的属性

## [创建索引的技巧](https://www.kancloud.cn/thinkphp/mysql-design-optimalize/39319)

1.维度高的列创建索引

数据列中不重复值出现的个数,这个数量越高,维度就越高

> 如数据表中存在8行数据a ,b ,c,d,a,b,c,d这个表的维度为4
> 要为维度高的列创建索引,如性别和年龄,那年龄的维度就高于性别
> 性别这样的列不适合创建索引,因为维度过低

2.对 where,on,group by,order by 中出现的列使用索引

3.对较小的数据列使用索引,这样会使索引文件更小,同时内存中也可以装载更多的索引键

4.为较长的字符串使用**前缀索引**

5.不要过多创建索引,除了增加额外的磁盘空间外,对于DML操作的速度影响很大,因为其每增删改一次就得从新建立索引

6.使用**组合索引**,可以减少文件索引大小,在使用时速度要优于多个单列索引

## [什么样的sql不走索引](https://www.kancloud.cn/thinkphp/mysql-design-optimalize/39319)

```sql
SELECT `sname` FROM `stu` WHERE `age`+10=30;-- 不会使用索引,因为所有索引列参与了计算

SELECT `sname` FROM `stu` WHERE LEFT(`date`,4) <1990; -- 不会使用索引,因为使用了函数运算,原理与上面相同

SELECT * FROM `houdunwang` WHERE `uname` LIKE'后盾%' -- 走索引

SELECT * FROM `houdunwang` WHERE `uname` LIKE "%后盾%" -- 不走索引

-- 正则表达式不使用索引,这应该很好理解,所以为什么在SQL中很难看到regexp关键字的原因

-- 字符串与数字比较不使用索引;
CREATE TABLE `a` (`a` char(10));
EXPLAIN SELECT * FROM `a` WHERE `a`="1" -- 走索引
EXPLAIN SELECT * FROM `a` WHERE `a`=1 -- 不走索引

select * from dept where dname='xxx' or loc='xx' or deptno=45 --如果条件中有or,即使其中有条件带索引也不会使用。换言之,就是要求使用的所有字段,都必须建立索引, 我们建议大家尽量避免使用or 关键字

-- 如果mysql估计使用全表扫描要比使用索引快,则不使用索引
```

## [多表关联时的索引效率](https://www.kancloud.cn/thinkphp/mysql-design-optimalize/39319)

![img](http://images0.cnblogs.com/blog2015/487276/201506/141008456443019.jpg)
从上图可以看出,所有表的type为all,表示全表索引;也就是6_6_6,共遍历查询了216次;

**除第一张表示全表索引(必须的,要以此关联其他表),其余的为range(索引区间获得)**,也就是6+1+1+1,共遍历查询9次即可;

所以我们**建议在多表join的时候尽量少join几张表**,因为一不小心就是一个笛卡尔乘积的恐怖扫描,另外,我们还**建议尽量使用left join,以少关联多**.因为使用join 的话,第一张表是必须的全扫描的,以少关联多就可以减少这个扫描次数.

## [索引的缺点](https://www.kancloud.cn/thinkphp/mysql-design-optimalize/39319)

- **创建**维护索引需要时间
- 索引占用物理**空间**，聚簇索引占用空间更大
- 当对表中数据增删改时，需**维护**索引
- 不要盲目的创建索引,只为查询操作频繁的列创建索引,创建索引会使查询操作变得更加快速,但是会降低增加、删除、更新操作的速度,因为执行这些操作的同时会对索引文件进行重新排序或更新;
- 但是,**在互联网应用中,查询的语句远远大于DML的语句,甚至可以占到80%~90%,所以也不要太在意,只是在大数据导入时,可以先删除索引,再批量插入数据,最后再添加索引**;

## 索引的(类型)数据结构

* 二叉查找树
* B-Tree
  * 根节点至少包括两个孩子
  * 树中每个节点最多包含有m个孩子（m>=2）
  * 除根节点和叶节点外，其他每个节点至少有ceil(m/2)个孩子
  * 所有叶子节点都在同一层
* B+-Tree（Mysql）
  * 非叶子节点的子树指针与关键字个数相同
  * 非叶子节点的子树指针P[i],指向关键字值[K[i],K[i+1]]的子树
  * 非叶子节点仅用来索引，叶子节点存储数据
  * 所有叶子节点均有一个指针链接到下一个叶子节点
* Hash
  * 仅能满足“=”，“IN”，不能使用范围查询
  * 无法被用来避免数据的排序操作
  * 不能利用部分索引键查询
  * 不能避免表扫描
  * 遇到大量Hash值相等的情况后性能并不一定比B树索引高
* BitMap索引

## 密集索引和稀疏索引的区别

* 密集索引文件中的每个搜索码值都对应一个索引值
* 稀疏索引文件只为索引码的某些值建立索引项

## [聚簇索引和非聚簇索引的区别](https://juejin.im/post/5cdd701ee51d453a36384939)

* ![img](https://user-gold-cdn.xitu.io/2019/5/16/16ac10253b8748df?imageView2/0/w/1280/h/960/format/webp/ignore-error/1)
* 对于**聚簇索引表**来说（左图），**表数据是和主键一起存储的**，**主键索引的叶结点存储行数据(**包含了主键值)，**二级索引的叶结点存储行的主键值**。使用的是B+树作为索引的存储结构，非叶子节点都是索引关键字，但非叶子节点中的关键字中不存储对应记录的具体内容或内容地址。叶子节点上的数据是主键与具体记录(数据内容)
* 对于**非聚簇索引表**来说（右图），**表数据和索引是分成两部分存储**的，主键索引和二级索引存储上没有任何区别。使用的是B+树作为索引的存储结构，所有的节点都是索引，**叶子节点存储的是索引+索引对应的记录的数据**。
* 聚簇索引查询只用一次,**非聚簇索引需要回表查询多次**,通过覆盖索引也可以只查询一次
* **覆盖索引**
  * 一个查询语句的执行只用从索引中就能够取得,不必从数据表中读取,称为实现了索引的覆盖,避免查到索引后回表操作,减少IO提高效率

## 回表

* 我们有个主键为ID的索引，和一个普通name字段的索引，我们在普通字段上搜索：

  `select * from table where name = '丙丙'`

  执行的流程是先查询到name索引上的“丙丙”，然后找到他的id是2，最后去主键索引，找到id为2对应的值。

  回到主键索引树搜索的过程，就是**回表**。不过也有方法避免回表，那就是**覆盖索引**。

## [覆盖索引](https://www.cnblogs.com/chenpingzhao/p/4776981.html)

* 刚才我们是 select * ，查询所有的，我们如果只查询ID那，其实在Name字段的索引上就已经有了，那就不需要回表了。
* 覆盖索引可以减少树的搜索次数，提升性能，他也是我们在实际开发过程中经常用来优化查询效率的手段。
* 很多**联合索引**的建立，就是为了支持覆盖索引，特定的业务能极大的提升效率

## 联合索引

### 最左匹配原则

- mysql索引规则中要求复合索引要想使用第二个索引，必须先使用第一个索引。（而且第一个索引必须是等值匹配）。
- **列的排列顺序决定了可命中索引的列数**
- mysql会一直向右匹配直到遇到范围查询（>,<,between,like）就停止匹配
  - 比如a=3 and b=4 and c>5 and d=6 
  - 如果建立（a,b,c,d）顺序的索引，d是用不到索引的
  - 如果建立（a,b,d,c）的索引，则都可以用到，a,b,d的顺序可以任意调整。
  - =和in可以乱序，比如a=1 and b =2 and c = 3 建立(a,b,c)索引可以任意顺序，mysql的查询优化器会优化成索引可以识别的形式
- 当创建一个联合索引时,如(key1,key2,key3),相当于他建了(key1),(key1,key2),(key1,key2.key3)

#### [最左匹配原理](https://hacpai.com/article/1581343220693)

* 最左匹配原则都是针对联合索引来说的，所以我们可以从联合索引的原理来了解最左匹配原则。

* 我们都知道索引的底层是一颗 B+ 树，那么联合索引当然还是一颗 B+ 树，只不过联合索引的健值数量不是一个，而是多个。**构建一颗 B+ 树只能根据一个值来构建**，因此数据库依据**联合索引最左的字段来构建 B+ 树**。

* 在 InnoDB 中联合索引只有先确定了前一个（左侧的值）后，才能确定下一个值。如果有范围查询的话，那么联合索引中使用范围查询的字段后的索引在该条 SQL 中都不会起作用。

* 值得注意的是，in 和 = 都可以乱序，比如有索引（a,b,c）,语句 `select * from t where c =1 and a=1 and b=1`,这样的语句也可以用到最左匹配，因为 MySQL 中有一个优化器，他会分析 SQL 语句，将其优化成索引可以匹配的形式，即 `select * from t where a =1 and a=1 and c=1`

* 

  ![image.png](https://i.loli.net/2020/04/10/5ABZJRLh9bKVwte.png)

  

### 索引失效

- 若条件中有or,即使其中有条件带索引也不会使用(尽量少使用or的原因)
- 对于多列索引,不是使用的第一部分,则不会使用索引
- like查询是以%开头
- 如果列类型是字符串,要在条件中使用绰号引起来,否则不会使用索引
- 如果MySQL估计使用全表扫描比索引快,则不使用索引

## 定位并优化慢查询Sql

* 根据慢日志定位慢查询sql
* 修改sql或者尽量让sql走索引
* 使用explain等工具分析sql
  * `type`表中找到所需行的方式
    * `ALL` `index` `range` `ref` `eq_ref` `const` `system` `NULL`
  * `key` 查询中实际使用的索引，若没有使用索引则为NULL
  * `ref` 表的连接匹配条件，即哪些列或常量被用于查找索引列上的值
  * `table` select查询的表的名字

## 查询优化器

* 一条SQL的查询可有不同执行方案,需通过优化器进行选择成本最低方案
  * 根据搜索条件,找出最有可能使用的索引
  * 计算全表扫描的代价
  * 计算使用不同索引执行查询的代价
  * 对比各种方案的代价,找出成本最低的



## 为何性别不适合用索引

* 访问索引需额外IO开销,从索引中拿到的只是地址,想访问真正的数据还需对表进行IO,此时开销并不一定比直接对表扫描小

## [隐式转换](https://zhuanlan.zhihu.com/p/30955365)

* MySQL会根据需要自动将数字转换为字符串，将字符串转换数字。

### [如何避免隐式转换](https://zhuanlan.zhihu.com/p/30955365)

* 使用CAST函数显示转换

  * > SELECT 38.8, CAST(38.8 AS CHAR);

* 类型一致

  * 

# 锁模块

## MyISAM与InnoDB关于锁方面的区别是什么？

* MyISAM默认用的是表级锁，不支持行级锁
* InnoDB默认用的是行级锁，也支持表级锁
  * 走索引时，用行锁
  * 不走索引时，用表锁

## MyISAM适合的场景

* 频繁执行全表count
* 对数据进行增删改的频率不高，查询非常频繁
* 没有事务

## InnoDB适合的场景

* 数据增删改查频繁
* 可靠性要求比较高，要求支持事务

## [锁的分类](https://zhuanlan.zhihu.com/p/52678870)

![img](https://pic3.zhimg.com/80/v2-5cf8b96fdca1428e6f3cce863fdfa73e_720w.jpg)

### 乐观锁

* 通常乐观锁通过使用版本号/时间戳实现

  `update table set fields = #{fields},version=#{new_version} where id=#{id} and version = #{old_version}`

* 乐观锁与悲观锁不同的是，它是一种逻辑上的锁，而不需要数据库提供锁机制来支持

### [意向锁](https://juejin.im/post/5b85124f5188253010326360)

- 意向共享锁

  （intention shared lock, IS）：事务有意向对表中的某些行加

  共享锁

  （S锁）

  ```sql
  -- 事务要获取某些行的 S 锁，必须先获得表的 IS 锁。
  SELECT column FROM table ... LOCK IN SHARE MODE;
  复制代码
  ```

- 意向排他锁

  （intention exclusive lock, IX）：事务有意向对表中的某些行加

  排他锁

  （X锁）

  ```sql
  -- 事务要获取某些行的 X 锁，必须先获得表的 IX 锁。
  SELECT column FROM table ... FOR UPDATE;
  复制代码
  ```

即：`意向锁是有数据引擎自己维护的，用户无法手动操作意向锁`，在为数据行加共享 / 排他锁之前，InooDB 会先获取该数据行所在在数据表的对应意向锁。

* [意向锁的作用](https://www.zhihu.com/question/51513268)

  * 考虑这个例子：

  ```
  事务A锁住了表中的**一行**，让这一行只能读，不能写。
  
  之后，事务B申请**整个表**的写锁。
  
  如果事务B申请成功，那么理论上它就能修改表中的任意一行，这与A持有的行锁是冲突的。
  
  数据库需要避免这种冲突，就是说要让B的申请被阻塞，直到A释放了行锁。
  
  数据库要怎么判断这个冲突呢？
  
  step1：判断表是否已被其他事务用表锁锁表
  step2：判断表中的每一行是否已被行锁锁住。
  
  注意step2，这样的判断方法效率实在不高，因为需要遍历整个表。
  于是就有了意向锁。
  
  在意向锁存在的情况下，事务A必须先申请表的意向共享锁，成功后再申请一行的行锁。
  
  在意向锁存在的情况下，上面的判断可以改成
  
  step1：不变
  step2：发现表上有意向共享锁，说明表中有些行被共享行锁锁住了，因此，事务B申请表的写锁会被阻塞。
  
  注意：申请意向锁的动作是数据库完成的，就是说，事务A申请一行的行锁的时候，数据库会自动先开始申请表的意向锁，不需要我们程序员使用代码来申请。
  ```




## [行锁种类](https://www.cnblogs.com/zhoujinyi/p/3435982.html)

* 行锁就是一锁锁一行或者多行记录，mysql的**行锁是基于索引加载的**，所以行锁是要加在索引响应的行上，即命中索引

### [记录锁](https://zhuanlan.zhihu.com/p/52678870)

* 记录锁锁的是表中的某一条记录，记录锁的出现条件<span style="color:red">**必须是精准命中索引并且索引是唯一索引**</span>，如主键id，就像我们上面描述行锁时使用的sql语句图，在这里就挺适用的。

### [GAP锁(间隙锁)](https://zhuanlan.zhihu.com/p/52678870)

* **防止幻读,在事务读取数据间隙上锁,防止在这个时间内其他事务修改数据**
* 间隙锁的触发条件必然是命中索引的，<span style="color:red">当我们查询数据用范围查询而不是相等条件查询时，**查询条件命中索引**，并且**没有查询到符合条件的记录**，此时就会将查询条件中的范围数据进行锁定(即使是范围库中不存在的数据也会被锁定)</span>
* **间隙锁只会出现在可重复读的事务隔离级别中**
* 根据检索条件向左寻找最靠近检索条件的记录A作为左区间，向右寻找最靠近检索条件的记录值B作为右区间，即锁定的间隙为（A，B]
  * 防止间隙内有新数据被插入
  * 防止已存在的数据更新成间隙内的数

### [next-key锁（临键锁）](https://www.cnblogs.com/zhoujinyi/p/3435982.html)

* 1+2，锁定一个范围，并且锁定记录本身。对于行的查询，都是采用该方法，主要目的是解决幻读的问题。

* 间隙锁的触发条件是命中索引，范围查询没有匹配到相关记录。而临键锁恰好相反，临键锁的触发条件也是**查询条件命中索引**，不过，临键锁**有匹配到数据库记录**；

* 因为**InnoDB对于行的查询都是采用了Next-Key Lock的算法**，锁定的不是单个值，而是一个范围，按照这个方法是会和第一次测试结果一样。但是，**当查询的索引含有唯一属性的时候，Next-Key Lock 会进行优化，将其降级为Record Lock，即仅锁住索引本身，不是范围**。

  注意：通过主键或则唯一索引来锁定不存在的值，也会产生GAP锁定。

* gap锁与行锁的组合，<span style='color:red'>InnoDB中，更新非唯一索引对应的记录时会加上Next-Key锁，如果更新记录为空则只能加gap锁</span>

## [如何实现分布式锁](https://blog.csdn.net/weixin_33805152/article/details/93633536)

* 利用主键唯一的特性，如果有多个请求同时提交到数据库的话，数据库会保证只有一个操作可以成功，那么我们就可以认为操作成功的那个线程获得了该方法的锁，当方法执行完毕之后，想要释放锁的话，删除这条数据库记录即可。

* **创建task_lock表，注意key作为唯一主键**

* ![表结构](https://img-blog.csdnimg.cn/20190625172056420.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zMzgwNTE1Mg==,size_16,color_FFFFFF,t_70)

  ```java
  //实现原理是:遍历表中的所有记录.筛选出过期的key,然后删除
  //每个小时第52分钟执行一次
      @Scheduled(cron = "0 52 * * * ?")
      private void checkSynTaskKeyIsExpire() {
          logger.info("=========开始检查数据库分布式锁的过期时间=======");
          List<TaskLock> taskLocks = taskLockMapper.findAll();
          //筛选出过期的key
          List<TaskLock> taskLockList = taskLocks.stream().
                  filter(taskLock -> new Date().getTime() - taskLock.getUtime().getTime()
                          - taskLock.getTimeout() * 1000 > 0).collect(Collectors.toList());
          taskLockList.forEach(taskLock -> {
              taskLockMapper.deleteByPrimaryKey(taskLock.getKey());
          });
          logger.info("=========trs_task_lock 删除{}条 过期的key=======",taskLockList.size());
      }
  
  ```

  

# 事务

## 为何要有事务

* 保证数据的最终一致性

## [事务的特性](https://blog.csdn.net/l1394049664/article/details/81814090)

* 原子性
  * **事务包含的操作要么全部成功,要么全部失败回滚,如果成功则必须完全应用到数据库,如果失败,则不能对数据库有任何影响**
* 一致性
  * 事务必须使数据库从一个一致性状态变换到另一个一致性状态，也就是说**一个事务执行之前和执行之后都必须处于一致性状态**
  * 拿转账来说，假设用户A和用户B两者的钱加起来一共是5000，那么不管A和B之间如何转账，转几次账，事务结束后两个用户的钱相加起来应该还得是5000，这就是事务的一致性。
* 持久性
  * **持久性是指一个事务一旦被提交了，那么对数据库中的数据的改变就是永久性的**，即便是在数据库系统遇到故障的情况下也不会丢失提交事务的操作。
* 隔离性
  * 隔离性是当多个用户并发访问数据库时，比如操作同一张表时，数据库为每一个用户开启的事务，不能被其他事务的操作所干扰，多个并发事务之间要相互隔离。
  * 对于任意两个并发的事务T1和T2，在事务T1看来，T2要么在T1开始之前就已经结束，要么在T1结束之后才开始，这样每个事务都感觉不到有其他事务在并发地执行。

## 如何保证事务的隔离性

* 加锁
* **频繁的加锁会带来什么问题**
  * 读数据的时候没办法修改。修改数据的时候没办法读取，极大的降低了数据库性能。
* **数据库是如何解决加锁后的性能问题的**
  * MVCC 多版本控制实现读取数据不用加锁， 可以让读取数据同时修改。修改数据时同时可读取。

## 事务隔离级别

* 可序列化
  * 最高隔离级别,强制事务串行执行,避免了幻读问题
  * 可序列化在读取的每一行数据上加锁,可能导致大量超时和锁争用问题
* 可重复读（默认）
  * **解决了脏读,不可重复读的发生**,保证多次读取同样记录结果是一样的,但无法解决幻读
  * 幻读:当某事务在读取某个范围内记录时,另一个事务又在该范围内插入了新的记录,当之前的事务再次读取该范围记录时,会产生幻行,通过多版本并发控制(MVCC)解决
* 提交读
  * 大多数数据库是默认提交读,MySQL不是
  * 一个事务从开始直到提交前,所做的任何修改对其他事务不可见,有时也叫不可重复读,因为两次查询的结果可能不同
  * **可避免脏读的发生**
* 未提交读
  * **事务可以读取未提交的数据(脏读**),实际中很少使用,任何情况都无法保证

## [事务隔离级别引起的问题](https://www.cnblogs.com/fjdingsd/p/5273008.html)

* 更新丢失
  
  * 两个事务对同一数据进行修改，后修改的覆盖先修改的
  
* 脏读
  
  * 事务1中修改数据未提交,被其他事务读到,然后事务1回滚,此时读的数据为脏数据
  
* 不可重复读
  
  * 指在对于数据库中的某个数据，一个事务范围内多次查询却返回了不同的数据值，这是由于在查询间隔，被另一个事务修改并提交了
  * 例如事务T1在读取某一数据，而事务T2立马修改了这个数据并且提交事务给数据库，事务T1再次读取该数据就得到了不同的结果，发送了不可重复读。
  
* 幻读
  
  * 第一个事务对一个表中的数据进行了修改，这种修改涉及到表中的全部数据行。 同时，第二个事务也修改这个表中的数据，这种修改是向表中**插入一行新数据**。那么，以后就会发生操作第一个事务的用户发现表中还有没有修改的数据行，就好象 发生了幻觉一样。
  
* **幻读和不可重复读都是读取了另一条已经提交的事务**（这点就脏读不同），所不同的是不可重复读查询的都是同一个数据项，而幻读针对的是一批数据整体（比如数据的个数）。

* 关键语法

  * GROUP BY
    * 查询所有同学的学号，选课数，总成绩

  ```SQL
  select student_id,count(course_id),sum(score) 
  from score 
  group by student_id 
  ```

  * 查询所有同学的学号，姓名，选课数，总成绩

  ```sql
    select s.student_id,stu.name,count(s.course_id),sum(s.score) 
    from 
    	score s ,
    	student stu
    where s.student_id = stu.student_id
    group by s.student_id; 
  ```

* HAVING

  * 查询平均成绩大于60分的同学的学号和平均成绩

  ```sql
  select student_id,avg(score)
  from score
  group by student_id
  having avg(score)>60;
  ```

* 统计相关

  * COUNT
  * SUM
  * MAX
  * MIN
  * AVG

查询没有学全所有课的同学的学号，姓名

```sql
select stu.student_id,stu.name
from 
	student stu,
	score s
where stu.student_id = s.student_id
group by student_id
having count(*) <
(
    select count(*) from course
)
```

# 数据库范式

* 1NF
  * 每个属性都不可再分
* 2NF
  * 属性完全依赖于主键（消除部分子函数依赖）
* 3NF
  * 属性不依赖于其它非主属性（消除传递依赖）
* BCNF
  * 在1NF基础上，任何非主属性不能对主键子集依赖（在3NF基础上消除对主码子集的依赖）
* 4NF
  * 要求把同一表内的多对多关系删除
* 5NF
  * 从最终结构重新建立原始结构

# 数据库连接池原理

* 连接池的建立
  * 系统初始化时根据系统配置建立连接池，并在池中建立几个连接对象
  * Java中可使用Vector,Stack建立连接池
* 连接池管理
  * 客户请求数据库连接
    * 查看连接池中是否有空闲连接，若有则分配给客户；若无，则查看当前连接是否到最大连接数，若否则新建连接给客户；若是则按设定最大等待时间等待，若走出最大等待时间，则抛异常给客户
  * 当客户释放连接时
    * 判断连接引用次数是否超过规定值，若超过则从连接池中删除，否则保留
* 连接池关闭
  * 程序退出时，关闭连接池中所有连接，释放连接池相关资源

# 分库分表

* 概念
  * 分库
    * 用户id直接mod分成库的数目大小，将大库分成小库
  * 分表
    * 用户 id 直接 mod 分成表的数目大小， 将大表拆成小表
  * 分库分表
    * 方式1
      * 中间变量=`user_id %(分库数量*每个库的表数量)`
      * `库 = 取整数（中间变量/每个库的表数量）`
      * `表 = 中间变量 % 每个库的表数量`
    * 垂直切分
      * 单机的ACID被打破，数据到多机后，原来单机通过事务来进行处理逻辑会受很大影响
      * Join操作困难，因数据可能在两个数据库中了，不能方便利用数据库自身join
      * 靠外键进行约束的场景会受到影响
    * 水平切分
      * ACID被打破
      * Join操作受影响
      * 靠外键约束场景受影响
      * 依赖单库的自增序列生成唯一ID会受影响
      * 针对单个逻辑意义上的表的查询要跨库

## 如何解决分库分表带来的坏处

* ACID解决方法
  * 两阶段提交
    * 事务在第一阶段对资源进行准备，若在准备阶段有一个资源失败，那么在第二阶段的处理就是回滚所有资源，否则进行Commit操作
* 水平切分自增ID破坏
  * 将所有ID集中放在一个地方管理，对每个ID序列独立管理，每台机器使用ID时都从这个ID造成器上进行获取
* 跨库Join
  * 在应用层将原来数据库中Join操作分成多次数据库操作
  * 数据冗余
    * 将常用信息进行冗余，将原来需要Join的返回信息变为单表查询
  * 借助外部系统斛同跨库问题
    * 如搜索引擎
* 外键约束
  * 需要分库后的每个单库的数据是内聚的，否则就只能靠应用层的判断，容错方式

# 解决加锁后的性能问题:MVCC

* 概念

  * <span style='color:red'>为了提供更好的并发，InnoDB提供了非锁定读：不需要等待访问行上的锁释放，读取行的一个快照。</span>>该方法是通过InnoDB的一个特性：MVCC来实现的
  * 同一份数据临时保留多版本的一种方式，进而实现并发控制
  * <span style='color:red'>为了提供更好的并发，InnoDB提供了非锁定读：不需要等待访问行上的锁释放，读取行的一个快照。</span>该方法是通过InnoDB的一个特性：MVCC来实现的
  * 同一份数据临时保留多版本的一种方式，进面实现并发控制
* 解决的问题

  * 同时读写数据库时，读数据的人会看到不一致数据

* 实现原理

  * 通过每行记录后面保存的两个隐藏的列实现，一个保存了行的删除时间，一个保存了系统版本号

  * 开始时系统版本号会作为事务版本号

  * INSERT - InnoDB为插入的每一行保存当前系统版本号作为行版本号。

    DELETE - InnoDB为删除的每一行保存当前系统版本号作为行删除标识。

    UPDATE - InnoDB为插入一行新纪录，保存当前系统版本号作为行版本号，同时，保存当前系统版本号到原来的行作为行删除标识。

* 优点

  * **保存两个额外系统版本号，使大多读操作都可以不用加锁**，这样的设计使得读数据操作很简单

* 缺点

  * 每行记录都需要额外存储空间，需要做更多的行检查工作，以及额外的维护工作

## MVCC解决了什么问题

* MVCC的实现，是通过保存数据在某个时间点的快照来实现的。也就是说，不管需要执行多长时间，每个事务看到的数据是一致的。**根据事务开始的时间不同，每个事物对同一张表，同一时刻看到的数据可能是不一样的。**
* InnoDB的MVCC是通过在每行记录后面保存两个隐藏的列来实现。这两个列，**一个保存了行的创建时间，一个保存了行的过期时间（删除时间）**。并且存储的并不是真实的时间值，而是系统版本号（system version number）。每开始一个新的事务，系统版本号都会自动递增。事务开始时刻的系统版本号会作为事务的版本号，用来和查询到的每行记录的版本号进行比较。
* 保存着两个额外的系统版本号，使大多数读操作都可以不用加锁。这样设计使得读操作简单，性能强，并且保证只会读取到符合标准的行。不足之处是没行记录都需要额外的存储空间，需要做更多的检查工作，以及一些额外的维护工作。
* **MVCC只在REPEATABLE READ和READ COMMITTED两个隔离级别下工作。其他两个隔离级别都和MVCC不兼容，因为READ UNCOMMITTED总是读取最新的数据行，而不是符合当前事务版本的数据行，而SERIALIZABLE会对所有读取到的行都加锁**。
* **简而言之就是解决了在REPEATABLE READ和READ COMMITTED两个隔离级别下读同一行和写同一行的两个事务的并发。**
* Read Committed - 一个事务读取数据时总是读这个数据最近一次被commit的版本
* Repeatable Read - 一个事务读取数据时总是读取当前事务开始之前最后一次被commit的版本（所以底层实现时需要比较当前事务和数据被commit的版本号）。

**举个简单的例子：**

1. **一个事务A（txnId=100）修改了数据X，使得X=1，并且commit了**
2. **另外一个事务B（txnId=101）开始尝试读取X，但是还X=1。但B没有提交。**
3. **第三个事务C（txnId=102）修改了数据X，使得X=2。并且提交了**
4. **事务B又一次读取了X。这时**

- <span style='color:red'>**如果事务B是Read Committed。那么就读取X的最新commit的版本，也就是X=2**</span>
- **如果事务B是Repeatable Read。那么读取的就是当前事务（txnId=101）之前X的最新版本，也就是X被txnId=100提交的版本，即X=1。**

## MVCC隔离级别

* MVCC是实现InnoDB存储引擎实现隔离级别的一种具体方式
  * 提交读
  * 可重复读

## 在读写并发过程中如何实现多版本

## 读写并发之后如何删除旧版本

# 数据库优化

## [数据库优化的几个阶段](https://www.cnblogs.com/rjzheng/p/9619855.html)

### 优化SQL和索引

(1)[用慢查询日志定位执行效率低的`SQL`语句](https://blog.csdn.net/why444216978/article/details/80447943)

* 默认情况下MySQL默认为10s才是慢查询

* 为了能够记录慢查询，我把这个慢查询的默认时间修改成1s

  ```mysql
  mysql> set long_query_time=1;
  mysql>show variables like ' long_query_time'
  ```


(2)用`explain`分析`SQL`的执行计划

(3)确定问题，采取相应的优化措施，建立索引啊，等

###  搭建缓存

### 读写分离

缓存也搞不定的情况下，搞主从复制，上读写分离。在应用层，区分读写请求。或者利用现成的中间件mycat或者altas等做读写分离。
需要注意的是,只要你敢说你用了主从架构，有三个问题，你要准备:

(1)主从的好处？

回答:实现数据库备份，实现数据库负载均衡，提交数据库可用性

(2)主从的原理?

回答:如图所示（图片不是自己画的，偷懒了）

![image](https://www.cnblogs.com/images/cnblogs_com/rjzheng/1281019/o_youhua1.jpg)

3)如何解决主从一致性?

回答:这个问题，我不建议在数据库层面解决该问题。根据CAP定理，主从架构本来就是一种高可用架构，是无法满足一致性的
哪怕你采用同步复制模式或者半同步复制模式，都是弱一致性，并不是强一致性。所以，推荐还是利用缓存，来解决该问题。

步骤如下:

1、自己通过测试，计算主从延迟时间，建议mysql版本为5.7以后，因为mysql自5.7开始，多线程复制功能比较完善，一般能保证延迟在1s内。不过话说回来，mysql现在都出到8.x了，还有人用5.x的版本么。

2、数据库的写操作，先写数据库，再写cache，但是有效期很短，就比主从延时的时间稍微长一点。

3、读请求的时候，先读缓存，缓存不存在(这时主从同步已经完成)，再读数据库。

### [垂直拆分](https://www.kancloud.cn/thinkphp/mysql-design-optimalize/39326)

上面四个阶段都没搞定，就来垂直拆分了。垂直拆分的复杂度还是比水平拆分小的。将你的表，按模块拆分为不同的小表。大家应该都看过《大型网站架构演变之路》，这种类型的文章或者书籍，基本都有提到这一阶段。
如果你有幸能够在什么运营商、银行等公司上班，你会发现他们一个表，几百个字段都是很常见的事情。所以，应该要进行拆分，拆分原则一般是如下三点:

(1)把不常用的字段单独放在一张表。

(2)把常用的字段单独放一张表

(3)经常组合查询的列放在一张表中（联合索引）。

### [水平拆分](https://www.kancloud.cn/thinkphp/mysql-design-optimalize/39326)

## [表的垂直拆分与水平拆分](https://www.kancloud.cn/thinkphp/mysql-design-optimalize/39326)

### 垂直拆分

* 通常我们按以下原则进行垂直拆分:

  1. 把不常用的字段单独放在一张表;
  2. 把text，blob等大字段拆分出来放在附表中;
  3. 经常组合查询的列放在一张表中;

  > 垂直拆分更多时候就应该在数据表设计之初就执行的步骤，然后查询的时候用jion关键起来即可;

### 水平拆分

* 水平拆分是指数据表**行的拆分**，**表的行数超过200万行时**，就会变慢，这时可以把一张的表的数据拆成多张表来存放。

#### 拆分原则

* 通常情况下，我们使用取模的方式来进行表的拆分;比如一张有400W的用户表`users`，为提高其查询效率我们把其分成4张表`users1，users2，users3，users4`
  通过用ID取模的方法把数据分散到四张表内`Id%4+1 = [1,2,3,4]`
  然后查询,更新,删除也是通过取模的方法来查询

* 在insert时还需要一张临时表`uid_temp`来提供自增的ID,**该表的唯一用处就是提供自增的ID;**

  ``` sql
  insert into uid_temp values(null);
  ```

  > 注意,进行水平拆分后的表,字段的列和类型和原表应该是相同的,但是要记得去掉auto_increment自增长

* **另外**

  - 部分业务逻辑也可以通过地区，年份等字段来进行归档拆分;
  - 进行拆分后的表，只能满足部分查询的高效查询需求，这时我们就要在产品策划上，从界面上约束用户查询行为。比如我们是按年来进行归档拆分的,这个时候在页面设计上就约束用户必须要先选择年,然后才能进行查询;
  - 在做分析或者统计时，由于是自己人的需求,多点等待其实是没关系的,并且并发很低,这个时候可以用union把所有表都组合成一张视图来进行查询,然后再进行查询;

  ```sql
  Create view users as select from users1 union select from users2 union.........
  ```

## [谈谈SQL慢查询的解决思路](https://juejin.im/post/5982b6496fb9a03c476d6d1d)

### 慢SQL的系统表现

**1，数据库CPU负载高。**一般是查询语句中有很多计算逻辑，导致数据库cpu负载。

**2，IO负载高导致服务器卡住。**这个一般和全表查询没索引有关系。

**3，查询语句正常，索引正常但是还是慢。**如果表面上索引正常，但是查询慢，需要看看是否索引没有生效。

### 开启SQL慢查询的日志

要开启日志，需要在 MySQL 的配置文件 my.cnf 的 [mysqld] 项下配置慢查询日志开启，如下所示：

```bash
[mysqld]slow_query_log=1
slow_query_log_file=/var/log/mysql/log-slow-queries.log
long_query_time=2
```



# [SQL四种语言](https://www.cnblogs.com/henryhappier/archive/2010/07/05/1771295.html)

* DDL **数据库定义语言**

  * DDL不需要commit

    ```
    CREATE
    ALTER
    DROP
    TRUNCATE
    COMMENT
    RENAME
    ```

* DML **数据操纵语言**

  * 由DBMS提供，用于让用户或程序员使用，实现对数据库中数据的操作。

    ```
    SELECT
    INSERT
    UPDATE
    DELETE
    MERGE
    CALL
    EXPLAIN PLAN
    LOCK TABLE
    ```

* DCL **数据库控制语言**

  * GRANT 授权
    REVOKE 取消授权

* TCL **事务控制语言**

  * SAVEPOINT 设置保存点
    ROLLBACK  回滚
    SET TRANSACTION

# [MySQL原子性如何保证](https://juejin.im/post/5e6599b1f265da572017fbc3)

## bin log

* `binlog`记录了数据库表结构和表数据变更，比如`update/delete/insert/truncate/create`。它不会记录`select`（因为这没有对表没有进行变更）
* `binlog`无论MySQL用什么引擎，都会有的。

## redo log

* `redo log`是MySQL的InnoDB引擎所产生的。

### bin log和redo log的区别

* 存储的内容

  `binlog`记载的是`update/delete/insert`这样的SQL语句，而`redo log`记载的是物理修改的内容（xxxx页修改了xxx）。

* 功能

  * `redo log`的作用是为**持久化**而生的。写完内存，如果数据库挂了，那我们可以通过`redo log`来恢复内存还没来得及刷到磁盘的数据，将`redo log`加载到内存里边，那内存就能恢复到挂掉之前的数据了。

  * `binlog`的作用是复制和恢复而生的。

    - 主从服务器需要保持数据的一致性，通过`binlog`来同步数据。
    - 如果整个数据库的数据都被删除了，`binlog`存储着所有的数据变更情况，那么可以通过`binlog`来对数据进行恢复。

  * `redo log`**事务开始**的时候，就开始记录每次的变更信息，而`binlog`是在**事务提交**的时候才记录。
    * 如果写`redo log`失败了，那我们就认为这次事务有问题，回滚，不再写`binlog`。
    * 如果写`redo log`成功了，写`binlog`，写`binlog`写一半了，但失败了怎么办？我们还是会对这次的**事务回滚**，将无效的`binlog`给删除（因为`binlog`会影响从库的数据，所以需要做删除操作）
    * <span style="color:green">如果写`redo log`和`binlog`都成功了，那这次算是事务才会真正成功。</span>

## undo log

* `undo log`主要有两个作用：回滚和多版本控制(MVCC)
* `undo log`主要存储的也是逻辑日志，比如我们要`insert`一条数据了，那`undo log`会记录的一条对应的`delete`日志。我们要`update`一条记录时，它会记录一条对应**相反**的update记录。

* 因为`undo log`存储着修改之前的数据，相当于一个**前版本**，MVCC实现的是读写不阻塞，读的时候只要返回前一个版本的数据就行了。

# 主从同步

## [实现MySQL数据库的实时备份](https://www.cnblogs.com/wu-jian/p/9396739.html)

1. Master中的所有数据库变更事件写入Binary Log文件
2. 当在Slave中执行“SLAVE START”命令时，开启Slave I/O Thread，并连接Master
3. Master侦测到Slave I/O Thread的连接，开启Log Jump Thread进行响应
4. Master Binary Log经Master Log Jump Thread和Slave I/O Thread传输至Slave Relay Log 
5. Slave SQL Thread将Relay Log还原至数据，同步完成

注：可使用“SHOW PROCESSLIST”命令在Master和Slave中查看对应线程的运行情况

## [MySQL 主从同步延时问题](https://github.com/doocs/advanced-java/blob/master/docs/high-concurrency/mysql-read-write-separation.md)

### **半同步复制**

* 用来解决主库数据丢失问题
* 主库写入 binlog 日志之后，就会将**强制**此时立即将数据同步到从库，从库将日志写入自己本地的 relay log 之后，接着会返回一个 ack 给主库，主库接收到**至少一个从库**的 ack 之后才会认为写操作完成了。

### **并行复制**

* 用来解决主从同步延时问题。
* 指的是从库开启多个线程，并行读取 relay log 中不同库的日志，然后**并行重放不同库的日志**，这是库级别的并行。

## [MySQL出现主从同步延迟可能有哪些原因](https://blog.csdn.net/yuki5233/article/details/75909858)

1.从库太多导致复制延迟
优化：建议从库数量3-5个为宜。

2.从库硬件比主库硬件差
优化：提升硬件性能。

3.慢SQL语句过多
优化：SQL语句执行时间太长，需要优化SQL语句。

4.主从复制的设计问题
优化：主从复制单线程，可以通过多线程IO方案解决；另外MySQL5.6.3支持多线程IO复制。

5.主从库之间的网络延迟
优化：尽量链路短，提升端口带宽。

6.主库读写压力大
优化：前端加buffer和缓存。主从延迟不同步： 不管有多延迟，只要不影响业务就没事。

7.业务设计缺陷导致延迟影响业务
优化：从库没有数据改读主库。

# [drop、truncate和delete](https://blog.csdn.net/shadow_zed/article/details/78252494)

|                  | drop                   | truncate                                                     | delete                                                       |
| ---------------- | ---------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 日志与事务       | 删除表结构             | 删除表中所有数据 且不记入日志保存，在删除的过程不会激活与表有关的删除触发器，执行速度快 | 每次从表中删除一行，同时将删除操作作为事务记录在日志中保存以便进行回滚 |
| 表和索引所占空间 | 表所占用的空间全释放掉 | 表和索引所占用的空间会恢复到初始大小                         | 不会减少表或索引所占用的空间                                 |
| 执行速度         |                        | drop > truncate > delete                                     |                                                              |
| 应用范围         |                        | 只能对table                                                  | table和view                                                  |

# [如何预防SQL注入](https://github.com/astaxie/build-web-application-with-golang/blob/master/zh/09.4.md)

* **严格限制Web应用的数据库的操作权限**，给此用户提供仅仅能够满足其工作的最低权限，从而最大限度的减少注入攻击对数据库的危害。
* **检查输入的数据是否具有所期望的数据格式，严格限制变量的类型**，例如使用regexp包进行一些匹配处理，或者使用strconv包对字符串转化成其他基本类型的数据进行判断。
* **对进入数据库的特殊字符（'"\尖括号&*;等）进行转义处理，或编码转换**。Go 的text/template包里面的HTMLEscapeString函数可以对字符串进行转义处理。
* 所有的查询语句建议使用**数据库提供的参数化查询接口**，参数化的语句使用参数而不是将用户输入变量嵌入到SQL语句中，即不要直接拼接SQL语句。例如使用database/sql里面的查询函数Prepare和Query，或者Exec(query string, args ...interface{})。
* 在应用发布之前建议**使用专业的SQL注入检测工具进行检测**，以及时修补被发现的SQL注入漏洞。网上有很多这方面的开源工具，例如sqlmap、SQLninja等。
* **避免网站打印出SQL错误信息**，比如类型错误、字段不匹配等，把代码里的SQL语句暴露出来，以防止攻击者利用这些错误信息进行SQL注入。

# [范式](https://www.kancloud.cn/thinkphp/mysql-design-optimalize/39324)

## 第一范式（1NF）

即**表的列的具有原子性**,不可再分解，即列的信息，不能分解, 只要数据库是关系型数据库(mysql/oracle/db2/informix/sysbase/sql server)，就自动的满足1NF。

## 第二范式（2NF）

第二范式（2NF）是在第一范式（1NF）的基础上建立起来的，即满足第二范式（2NF）必须先满足第一范式（1NF）。第二范式（2NF）要求数据库表中的每个实例或行必须可以被惟一地区分。**为实现区分通常需要我们设计一个主键来实现(这里的主键不包含业务逻辑)**

## 第三范式（3NF）

满足第三范式（3NF）必须先满足第二范式（2NF）。简而言之，第三范式（3NF）要求一个数据库表中不包含已在其它表中已包含的非主键字段。就是说，**表的信息，如果能够被推导出来，就不应该单独的设计一个字段来存放(能尽量外键join就用外键join)。很多时候，我们为了满足第三范式往往会把一张表分成多张表**

![img](http://images0.cnblogs.com/blog2015/487276/201505/191142168389020.png)

## 反三范式

没有冗余的数据库未必是最好的数据库，有时为了提高运行效率，就必须降低范式标准，适当保留冗余数据。具体做法是： 在概念数据模型设计时遵守第三范式，降低范式标准的工作放到物理数据模型设计时考虑。降低范式就是增加字段，减少了查询时的关联，提高查询效率，因为在数据库的操作中查询的比例要远远大于DML的比例。但是反范式化一定要适度，并且在原本已满足三范式的基础上再做调整的。