# MapReduce 的工作原理

MapReduce中，Map是一个将输入记录转换为中间记录的任务。已转换的中间记录不一定非得与输入记录具有相同的类型。一个给定的输入对可能映射到零个或多个输出对。Map数量通常由输入数据的大小决定，也就是输入数据的总的块数【(MRJobConfig.NUM_MAPS可设置】。Reducer接收的数据是Mapper中处理出来的已经排好序的数据。 

**在MapReduce过程中需要将各个节点上的同一类数据汇集到一个节点进行计算。把这些分布在不同节点的数据按照一定规则聚集到一起的过程，就称之为shuffle。**

* ## 4个阶段

* Split（分片输入）

* Map

  * 增加map任务个数

    增大：mapred.map.tasks

  * 减少map个数.增大mapred.min.split.size

  * 若要减少map个数，但有很多小文件，可将小文件合并成大文件，再使用准则2

* Combine

  * 可认为Combine是对本地reduce操作

* Shuffle

* Reduce

**Map数量：** 对于大文件：由任务的 split 切片决定的，**一个 split 切片对应一个map任务**。先明确一点 split 切片的大小可自己配置的，一般来说对于大文件会选择split == block，如果split < block的情况下会增加 map 的数量，虽然这样可以增加map执行的并行度，但是会导致map任务在不同节点拉取数据，浪费了网络资源等。ps:HDFS 中 block 是最小的存储单元，默认128M。  对于小文件：由参与任务的文件数量决定，默认情况一个小文件启动一个 map 任务，小文件数量较多会导致启动较大数量的 map 任务，增加资源消耗。此时建议将多个小文件通过 **InputFormat 合并成一个大文件加入到一个 split 中**，并增加 split 的大小，这样可以有效减少 map 的数量，降低并发度，减少资源消耗。  

**Reduce数量**：由分区（partiton）的数量决定的，我们可以在代码中配置 job.setNumReduceTasks(*)来控制 reduce 的任务数量。 



## Shuffle 过程分为Map端跟Reduce端的Shuffle过程

### **Map端流程**

1. **环形内存缓存区**：每个split数据交由一个map任务处理，map的处理结果不会直接写到硬盘上，会先输送到环形内存缓存区中，默认的大小是100M（可通过配置修改），当缓冲区的内容达到80%后会开始溢出，此时缓存区的溢出内容会被写到磁盘上，形成一个个spill file，注意这个文件没有固定大小。
2. 在内存中经过分区、排序后溢出到磁盘：分区主要功能是用来指定 map 的输出结果交给哪个 reduce 任务处理，**默认是通过 map 输出结果的 key 值取hashcode 对代码中配置的 redue task数量取模运算**，值一样的分到一个区，也就是一个 reduce 任务对应一个分区的数据。这样做的好处就是可以避免有的 reduce 任务分配到大量的数据，而有的 reduce 任务只分配到少量甚至没有数据，平均 reduce 的处理能力。并且在每一个分区（partition）中，都会有一个 sort by key 排序，如果此时设置了 Combiner，将排序后的结果进行 Combine 操作，相当于 map 阶段的本地 reduce，这样做的目的是让尽可能少的数据写入到磁盘。
3. **合并溢出文件**：随着 map 任务的执行，不断溢出文件，直到输出最后一个记录，可能会产生大量的溢出文件，这时需要对这些大量的溢出文件进行合并，在合并文件的过程中会不断的进行排序跟 Combine 操作，这样做有两个好处：**减少每次写入磁盘的数据量&减少下一步 reduce 阶段网络传输的数据量**。最后合并成了一个分区且排序的大文件，此时可以再进行配置压缩处理，可以减少不同节点间的网络传输量。合并完成后着手将数据拷贝给相对应的reduce 处理，那么要怎么找到分区数据对应的那个 reduce 任务呢？简单来说就是 **JobTracker 中保存了整个集群中的宏观信息**，只要 reduce 任务向 JobTracker 获取对应的 map 输出位置就可以了。具体请参考上方的MapReduce工作原理。

### **Reduce端流程**

reduce 会接收到不同 map 任务传来的有序数据，如果 reduce 接收到的数据较小，**则会存在内存缓冲区中，直到数据量达到该缓存区的一定比例时对数据进行合并后溢写到磁盘上**。随着溢写的文件越来越多，后台的线程会将他们合并成一个更大的有序的文件，可以为后面合并节省时间。这其实跟 map端的操作一样，**都是反复的进行排序、合并**，这也是 Hadoop 的灵魂所在，但是如果在 map 已经压缩过，在合并排序之前要先进行解压缩。合并的过程会产生很多中间文件，但是最后一个合并的结果是不需要写到磁盘上，而是可以直接输入到 reduce 函数中计算，每个 reduce 对应一个输出结果文件。 



 